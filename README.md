Preprocessing steps taken were to transform the text to lowercase, perform word tokenization, eliminate stopwords from the tokens, remove punctuation marks from the tokens, and eliminate empty space tokens. These were done for words excluding the phrase query where only the text was lowered. Thus the phrase query has stopwords in them that need to be kept. Some assumptions made were the phrase query or simple query was formed from the dictionary, as an example if a query is “lowercase words” then lowercase is spelt this way not lower-case or lower case. Another assumption is that the user wants the query to have stopwords and spaces, to make it easier for them to understand and interpret the output. However, the rest of the document is preprocessed is another assumption. The next assumption is that the positions in the positional index data structure and the document ids start at 0 and not 1. The last assumption made is that a user will input a query like the examples given in the lecture slides for positional index, like a single word for a query and a phrase query of 5 or less words. The methodology is functional programming because many built in functions are used in the code and some functions were created to be reused or for helping the readability of the code. A brief description of the outputs is that if the user’s query is a phrase then the positional index for the query phrase is given for only the query phrase and nothing else or else the full output would not be shown. Additionally, the output will also have the number of retrieved documents and the list of file names. If the query is not a phrase then the positional index will be the output along with the number of retrieved documents and the list of file names.<br>
<br>
For question 2, the preprocessing was the same, albeit done a bit differently. A regex pattern was used to define the tokens, and this immediately removed any non-alphanumeric characters. Then, the stopwords were removed in much the same way.<br>
<br>
Then, a function was made that constructs a pseudo-positional index, given the set of documents. It's different from a usual pasitional index in that it doesn't record the positions of a word in each document; only the count was kept track of. For example, one entry would be:<br>
&emsp;t1: df(t1), [f(t1, d1), f(t1, d2), ... f(t1, dn)]<br>
where "t1" represents the first term, and d1 to dn represents documents 1 to n. This function also returns a document-enumeration hashtable.<br>
<br>
The next step was to define the functions that would construct the TF-IDF matrices using the different weighting schemes. Using the positional index mentioned above, constructing the binary, raw-count and log-normalization matrices was quite straightforward. For the term-frequency weighting scheme, the function first generates a raw-count matrix and takes the column-wise summations. So for each term, we would have the list of frequencies in each document as a vector, divided by the summation vector. Finally, a similar approach was followed for the double-normalization weighting scheme, however instead of the summations, the column-wise maximums were used.<br>
<br>
The calculation of cosine similarity is trivial. The construction of the query vector was also trivial, as it was treated as another document (including preprocessing), and its TF-IDF vectors for all the different schemes were calculated in the manner mentioned above.<br>
<br>
That then leaves the function that gets the top 5 most similar documents. For a given weighting scheme, the cosine similarity between the query vector and each document vector was calculated. The similarities were kept track of in a dictionary, with the keys being the document number, and the values being the similarity. To get the top 5 most similar documents, the dictionary was sorted on its values, and the first 5 indices (5 most similar documents) were used to return the correspponding document names.<br>
<br>
As for assumptions, it was assumed that the query would be in a natural language form. It was also assumed that words with contractions would not be important, and that hyphenated words carry the same meaning regardless of hyphenation. This assumption was made due to the tokenization method used; the text treated non-alphanumeric characters as markers to tokenize. As such, words liks "can't" would be split up as "can" and "t". Such words were assumed unimportant as most contracted words are very commonly used words (the main reason for contraction), and as such did not add much meaning to the document text. Additionally, "first-class" would be split up into "first" and "class". In reality, the meaning of such terms isn't dependent on the hyphenation to any human reading the text, and that perspective is what was used to process such terms.