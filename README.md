<h1>Question 1</h1><br>
<h2>Methodology and Assumptions</h2><br>
Preprocessing steps taken were to transform the text to lowercase, perform word tokenization, eliminate stopwords from the tokens, remove punctuation marks from the tokens, and eliminate empty space tokens. These were done for words excluding the phrase query where only the text was lowered. Thus the phrase query has stopwords in them that need to be kept. Some assumptions made were the phrase query or simple query was formed from the dictionary, as an example if a query is “lowercase words” then lowercase is spelt this way not lower-case or lower case. Another assumption is that the user wants the query to have stopwords and spaces, to make it easier for them to understand and interpret the output. However, the rest of the document is preprocessed is another assumption. The next assumption is that the positions in the positional index data structure and the document ids start at 0 and not 1. The last assumption made is that a user will input a query like the examples given in the lecture slides for positional index, like a single word for a query and a phrase query of 5 or less words. The methodology is functional programming because many built in functions are used in the code and some functions were created to be reused or for helping the readability of the code. A brief description of the outputs is that if the user’s query is a phrase then the positional index for the query phrase is given for only the query phrase and nothing else or else the full output would not be shown. Additionally, the output will also have the number of retrieved documents and the list of file names. If the query is not a phrase then the positional index will be the output along with the number of retrieved documents and the list of file names.<br>
<br><h1>Question 2</h1>
<h2>Methodology and Assumptions</h2>
For question 2, the preprocessing was the same, albeit done a bit differently. A regex pattern was used to define the tokens, and this immediately removed any non-alphanumeric characters. Then, the stopwords were removed in much the same way.<br>
<br>
Then, a function was made that constructs a pseudo-positional index, given the set of documents. It's different from a usual pasitional index in that it doesn't record the positions of a word in each document; only the count was kept track of. For example, one entry would be:<br>
&emsp;&emsp;;t1: df(t1), [f(t1, d1), f(t1, d2), ... f(t1, dn)]<br>
where "t1" represents the first term, and d1 to dn represents documents 1 to n. This function also returns a document-enumeration hashtable.<br>
<br>
The next step was to define the functions that would construct the TF-IDF matrices using the different weighting schemes. Using the positional index mentioned above, constructing the binary, raw-count and log-normalization matrices was quite straightforward. For the term-frequency weighting scheme, the function first generates a raw-count matrix and takes the column-wise summations. So for each term, we would have the list of frequencies in each document as a vector, divided by the summation vector. Finally, a similar approach was followed for the double-normalization weighting scheme, however instead of the summations, the column-wise maximums were used.<br>
<br>
The calculation of cosine similarity is trivial. The construction of the query vector was also trivial, as it was treated as another document (including preprocessing), and its TF-IDF vectors for all the different schemes were calculated in the manner mentioned above.<br>
<br>
That then leaves the function that gets the top 5 most similar documents. For a given weighting scheme, the cosine similarity between the query vector and each document vector was calculated. The similarities were kept track of in a dictionary, with the keys being the document number, and the values being the similarity. To get the top 5 most similar documents, the dictionary was sorted on its values, and the first 5 indices (5 most similar documents) were used to return the correspponding document names.<br>
<br>
As for assumptions, it was assumed that the query would be in a natural language form. It was also assumed that words with contractions would not be important, and that hyphenated words carry the same meaning regardless of hyphenation. This assumption was made due to the tokenization method used; the text treated non-alphanumeric characters as markers to tokenize. As such, words liks "can't" would be split up as "can" and "t". Such words were assumed unimportant as most contracted words are very commonly used words (the main reason for contraction), and as such did not add much meaning to the document text. Additionally, "first-class" would be split up into "first" and "class". In reality, the meaning of such terms isn't dependent on the hyphenation to any human reading the text, and that perspective is what was used to process such terms.<br>
<h2>Comparison of Weighting Schemes</h2>
For the binary weighting scheme, one pro is that it is very simple to implement. However, it is impossible to factor for the repetition of words, which can affect document similarity. For example, if my query was "King of England", then a document with these terms appearing once or twice would be less relevant than a document that mentions these terms dozens of times.<br>
<br>
For the raw-count weighting scheme, it is also very easy to implement, and now we can use the repetition of terms to find more relevant documents. However, one drawback is that very frequent words often don't carry much meaning, and can overshadow less common terms in a document that carry more meaning. For example, the word "of" is used a lot compared to the word "democracy", but "of" gives us absolutely zero information about the document. One way to avoid this is to remove stopwords, but stopwords can prove crucial to meaning. For example, "from" and "to" are both stopwords, yet they are crucial in meaning in the phrases "USA to Canada" vs "USA from Canada". Additionally, the word "of" could very well skew the cosine similarity calculations, but to avoid the skew we again lose meaning.<br>
<br>
For the term-frequency weighting scheme, it honestly isn't that good of a weighting scheme as it is a version of the raw-count weighting scheme, but is harder to implement. By dividing the frequency of a single term by the summation of frequencies of all terms in a document, words that appear more will still have higher values than words that appear less. And so, the problem of stopwords remains. Words that don't have much meaning yet appear a lot could skew the similarity calculation, and if we were to remove such words, we would lose meaning and context information.<br>
<br>
The log-normalization weighting scheme is an improvement over the raw-count weighting scheme, while the term-frequency scheme was a downgrade. The advantages of the log-normalization scheme are that it is easy to implement, and since we convert the term-frequencies to a logarithmic scale, the skewed effect is reduced quite a bit. Not completely though - the logarithm is a monotonic transformation, and as such, if f(t1, d) > f(t2, d), then log(f(t1, d)) > log(f(t2, d)). As such, the issue of stopwords remains.<br>
<br>
The double-normalization weighting scheme is quite similar to the tern-frequency weighting scheme, except that it uses the maximum frequency of all terms in a document as the main normalizing factor, instead of the summation. In addition, this number is then divided in half again. The result is that the skewness is reduced quite a bit like the log-normalization scheme. However, it is more difficult to implement, like the term-frequency scheme.<br>